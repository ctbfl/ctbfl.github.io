<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/hjs.jpg"/><link rel="stylesheet" href="/_next/static/css/39cb4fde2fa41b5e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-1b36f25749a65f0c.js" async=""></script><script src="/_next/static/chunks/711-3ac098f4aa3de521.js" async=""></script><script src="/_next/static/chunks/217-414f3bc2f28e17e5.js" async=""></script><script src="/_next/static/chunks/862-15a393ad2462454b.js" async=""></script><script src="/_next/static/chunks/app/layout-04f81b10b1cda18d.js" async=""></script><script src="/_next/static/chunks/30a37ab2-4db9c7aaa3f9c255.js" async=""></script><script src="/_next/static/chunks/94730671-6c060cb276896248.js" async=""></script><script src="/_next/static/chunks/907-77394335ddd73061.js" async=""></script><script src="/_next/static/chunks/213-d1d263081572d555.js" async=""></script><script src="/_next/static/chunks/997-38646f922bcecd56.js" async=""></script><script src="/_next/static/chunks/748-7025ceaea1e8aacd.js" async=""></script><script src="/_next/static/chunks/app/page-cbb7215c4f2dc9db.js" async=""></script><link rel="icon" href="/fav.ico" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;family=Noto+Sans+SC:wght@300;400;500;600;700&amp;family=Noto+Serif+SC:wght@400;600;700&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;family=Noto+Sans+SC:wght@300;400;500;600;700&amp;family=Noto+Serif+SC:wght@400;600;700&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;family=Noto+Sans+SC:wght@300;400;500;600;700&amp;family=Noto+Serif+SC:wght@400;600;700&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Junshan Huang</title><meta name="description" content="PhD student at Rutgers University."/><meta name="author" content="Junshan Huang"/><meta name="keywords" content="Junshan Huang,PhD,Research,Rutgers University"/><meta name="creator" content="Junshan Huang"/><meta name="publisher" content="Junshan Huang"/><meta property="og:title" content="Junshan Huang"/><meta property="og:description" content="PhD student at Rutgers University."/><meta property="og:site_name" content="Junshan Huang&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Junshan Huang"/><meta name="twitter:description" content="PhD student at Rutgers University."/><link rel="icon" href="/fav.ico"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Junshan Huang</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/teaching/"><span class="relative z-10">Teaching</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/blog/"><span class="relative z-10">Blog</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R5pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Junshan Huang" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/hjs.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Junshan Huang</h1><p class="text-lg text-accent font-medium mb-1">PhD Student</p><p class="text-neutral-600 mb-2">Rutgers University</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?user=bPL1D18AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://github.com/ctbfl" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://space.bilibili.com/398516133" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Bilibili"><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" class="h-5 w-5" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858 0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234 0 0 1-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373.347 0 .653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836 0 0 1 .16-.213l2.853-2.747c.267-.249.573-.373.92-.373.347 0 .662.151.929.4.267.249.391.551.391.907 0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373 0 .684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373 0 .684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373Z"></path></svg></a><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="CV"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" class="h-5 w-5" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M21.0082 3C21.556 3 22 3.44495 22 3.9934V20.0066C22 20.5552 21.5447 21 21.0082 21H2.9918C2.44405 21 2 20.5551 2 20.0066V3.9934C2 3.44476 2.45531 3 2.9918 3H21.0082ZM20 5H4V19H20V5ZM18 15V17H6V15H18ZM12 7V13H6V7H12ZM18 11V13H14V11H18ZM10 9H8V11H10V9ZM18 7V9H14V7H18Z"></path></svg></button></div></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>Embodied AI</div><div>Kitchen Automation</div><div>Dexterous Manipulation</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am a PhD student at the School of Computer Science, Rutgers University, advised by <a href="https://arc-l.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. jingjin Yu</a>.</p>
<p class="mb-4 last:mb-0">Prior to this, I obtained a BSc degree with Honor Degree (top 5%) in Artificial Intelligence from the University of Science and Technology of China (USTC).</p>
<p class="mb-4 last:mb-0">My research focuses on developing more capable and reliable robot brains that allow robots to work efficiently and safely in household environments.</p>
<p class="mb-4 last:mb-0">The kitchen, as a compact yet highly representative domestic space, serves as an ideal starting point for this mission.</p>
<p class="mb-4 last:mb-0">I categorize kitchen-related tasks into three major groups:</p>
<ul class="list-disc list-inside mb-4 space-y-1 ml-4">
<li class="mb-1">Ingredient Preparation: Converting raw materials into ready-to-cook components.</li>
<li class="mb-1">Food Production: Assisting in or autonomously performing cooking tasks.</li>
<li class="mb-1">Post-Meal Tidying: Cleaning, dishwashing, and waste handling to maintain kitchen usability and hygiene.</li>
</ul>
<p class="mb-4 last:mb-0">Currently, I am focusing on solving post-meal tidying tasks.</p></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All â†’</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Chongkai Gao</span>, </span><span><span class="">Zixuan Liu</span>, </span><span><span class="">Zhenghao Chi</span>, </span><span><span class="font-semibold text-accent">Junshan Huang</span>, </span><span><span class="">Xin Fei</span>, </span><span><span class="">Yiwen Hou</span>, </span><span><span class="">Yuxuan Zhang</span>, </span><span><span class="">Yudi Lin</span>, </span><span><span class="">Zhirui Fang</span>, </span><span><span class="">Lin Shao</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Advances in Neural Information Processing Systems (NeurIPS)</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">Unified benchmarking reveals visually grounded hierarchical planning excels in VLAs.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Unifarn: Unified transformer for facial reaction generation</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Cong Liang</span>, </span><span><span class="">Jiahe Wang</span>, </span><span><span class="">Haofan Zhang</span>, </span><span><span class="">Bing Tang</span>, </span><span><span class="font-semibold text-accent">Junshan Huang</span>, </span><span><span class="">Shangfei Wang</span>, </span><span><span class="">Xiaoping Chen</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Proceedings of the 31st ACM International Conference on Multimedia (ACM MM)</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">Unified transformer predicts expressive facial reactions from multimodal conversational input.</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2026-02</span><p class="text-sm text-neutral-700">ðŸŽ‰ A paper was accepted by ICRA 2026!</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-09</span><p class="text-sm text-neutral-700">ðŸŽ‰ Our paper VLS-OS accepted by NeurIPS 2025!</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-09</span><p class="text-sm text-neutral-700">Starting my PhD at the Rutgers University~</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-06</span><p class="text-sm text-neutral-700">ðŸŽ“ Graduated from USTC with a honor degree! (Top 5%)</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-06</span><p class="text-sm text-neutral-700">Awarded the AI Talent Program Scholarship from USTC.</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2023-10</span><p class="text-sm text-neutral-700">Awarded the Shenzhen Stock Exchange Scholarship.</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2023-10</span><p class="text-sm text-neutral-700">We win the 1st place in the ACM REACT 2023 Multimodal Challenge!</p></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->February 14, 2026</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ðŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"711\",\"static/chunks/711-3ac098f4aa3de521.js\",\"217\",\"static/chunks/217-414f3bc2f28e17e5.js\",\"862\",\"static/chunks/862-15a393ad2462454b.js\",\"177\",\"static/chunks/app/layout-04f81b10b1cda18d.js\"],\"ThemeProvider\"]\n3:I[768,[\"711\",\"static/chunks/711-3ac098f4aa3de521.js\",\"217\",\"static/chunks/217-414f3bc2f28e17e5.js\",\"862\",\"static/chunks/862-15a393ad2462454b.js\",\"177\",\"static/chunks/app/layout-04f81b10b1cda18d.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"711\",\"static/chunks/711-3ac098f4aa3de521.js\",\"217\",\"static/chunks/217-414f3bc2f28e17e5.js\",\"862\",\"static/chunks/862-15a393ad2462454b.js\",\"177\",\"static/chunks/app/layout-04f81b10b1cda18d.js\"],\"default\"]\n7:I[3583,[\"362\",\"static/chunks/30a37ab2-4db9c7aaa3f9c255.js\",\"204\",\"static/chunks/94730671-6c060cb276896248.js\",\"711\",\"static/chunks/711-3ac098f4aa3de521.js\",\"907\",\"static/chunks/907-77394335ddd73061.js\",\"217\",\"static/chunks/217-414f3bc2f28e17e5.js\",\"213\",\"static/chunks/213-d1d263081572d555.js\",\"997\",\"static/chunks/997-38646f922bcecd56.js\",\"748\",\"static/chunks/748-7025ceaea1e8aacd.js\",\"974\",\"static/chunks/app/page-cbb7215c4f2dc9db.js\"],\"default\"]\n8:I[9507,[\"362\",\"static/chunks/30a37ab2-4db9c7aaa3f9c255.js\",\"204\",\"static/chunks/94730671-6c060cb276896248.js\",\"711\",\"static/chunks/711-3ac098f4aa3de521.js\",\"907\",\"static/chunks/907-77394335ddd73061.js\",\"217\",\"static/chunks/217-414f3bc2f28e17e5.js\",\"213\",\"static/chunks/213-d1d263081572d555.js\",\"997\",\"static/chunks/997-38646f922bcecd56.js\",\"748\",\"static/chunks/748-7025ceaea1e8aacd.js\",\"974\",\"static/chunks/app/page-cbb7215c4f2dc9db.js\"],\"default\"]\n9:I[5218,[\"362\",\"static/chunks/30a37ab2-4db9c7aaa3f9c255.js\",\"204\",\"static/chunks/94730671-6c060cb276896248.js\",\"711\",\"static/chunks/711-3ac098f4aa3de521.js\",\"907\",\"static/chunks/907-77394335ddd73061.js\",\"217\",\"static/chunks/217-414f3bc2f28e17e5.js\",\"213\",\"static/chunks/213-d1d263081572d555.js\",\"997\",\"static/chunks/997-38646f922bcecd56.js\",\"748\",\"static/chunks/748-7025ceaea1e8aacd.js\",\"974\",\"static/chunks/app/page-cbb7215c4f2dc9db.js\"],\"default\""])</script><script>self.__next_f.push([1,"]\nd:I[1990,[\"362\",\"static/chunks/30a37ab2-4db9c7aaa3f9c255.js\",\"204\",\"static/chunks/94730671-6c060cb276896248.js\",\"711\",\"static/chunks/711-3ac098f4aa3de521.js\",\"907\",\"static/chunks/907-77394335ddd73061.js\",\"217\",\"static/chunks/217-414f3bc2f28e17e5.js\",\"213\",\"static/chunks/213-d1d263081572d555.js\",\"997\",\"static/chunks/997-38646f922bcecd56.js\",\"748\",\"static/chunks/748-7025ceaea1e8aacd.js\",\"974\",\"static/chunks/app/page-cbb7215c4f2dc9db.js\"],\"default\"]\ne:I[9665,[],\"MetadataBoundary\"]\n10:I[9665,[],\"OutletBoundary\"]\n13:I[4911,[],\"AsyncMetadataOutlet\"]\n15:I[9665,[],\"ViewportBoundary\"]\n17:I[6614,[],\"\"]\n:HL[\"/_next/static/css/39cb4fde2fa41b5e.css\",\"style\"]\na:T5b6,Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual "])</script><script>self.__next_f.push([1,"learning ability, albeit at the cost of slower training and inference speeds.b:T7b6,@inproceedings{gao2025vlaos,\n  title = {VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models},\n  author = {Gao, Chongkai and Liu, Zixuan and Chi, Zhenghao and Huang, Junshan and Fei, Xin and Hou, Yiwen and Zhang, Yuxuan and Lin, Yudi and Fang, Zhirui and Shao, Lin},\n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n  year = {2025},\n  abstract = {Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.},\n  note = {San Diego, USA},\n  project = {https://nus-lins-lab.github.io"])</script><script>self.__next_f.push([1,"/vlaos/}\n}c:T45b,@inproceedings{liang2023unifarn,\n  title = {Unifarn: Unified transformer for facial reaction generation},\n  author = {Liang, Cong and Wang, Jiahe and Zhang, Haofan and Tang, Bing and Huang, Junshan and Wang, Shangfei and Chen, Xiaoping},\n  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia (ACM MM)},\n  pages = {9506--9510},\n  year = {2023},\n  abstract = {We propose the Unified Transformer for Facial Reaction GeneratioN (UniFaRN) framework for facial reaction prediction in dyadic interactions. Given the video and audio of one side, the task is to generate facial reactions of the other side. The challenge of the task lies in the fusion of multi-modal inputs and balancing appropriateness and diversity. We adopt the Transformer architecture to tackle the challenge by leveraging its flexibility of handling multi-modal data and ability to control the generation process. By successfully capturing the correlations between multi-modal inputs and outputs with unified layers and balancing the performance with sampling methods, we have won first place in the REACT2023 challenge.}\n}"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"BBzcyb289O2pMnVow-hAv\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/39cb4fde2fa41b5e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/fav.ico\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026family=Noto+Sans+SC:wght@300;400;500;600;700\u0026family=Noto+Serif+SC:wght@400;600;700\u0026display=swap\"}],[\"$\",\"link\",null,{\"suppressHydrationWarning\":true,\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026family=Noto+Sans+SC:wght@300;400;500;600;700\u0026family=Noto+Serif+SC:wght@400;600;700\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026family=Noto+Sans+SC:wght@300;400;500;600;700\u0026family=Noto+Serif+SC:wght@400;600;700\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Teaching\",\"type\":\"page\",\"target\":\"teaching\",\"href\":\"/teaching\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"Blog\",\"type\":\"page\",\"target\":\"blog\",\"href\":\"/blog\"}],\"siteTitle\":\"Junshan Huang\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"February 14, 2026\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Junshan Huang\",\"title\":\"PhD Student\",\"institution\":\"Rutgers University\",\"avatar\":\"/hjs.jpg\"},\"social\":{\"email\":\"junshan.huang@rutgers.edu\",\"location\":\"New Brunswick, NJ08901, USA\",\"location_url\":\"https://maps.app.goo.gl/WQS1qSpAoB4bMgmKA\",\"location_details\":[\"CoRE Building, Busch Campus\",\"Rutgers-New Brunswick, New Jersey, USA\"],\"google_scholar\":\"https://scholar.google.com/citations?user=bPL1D18AAAAJ\u0026hl=en\",\"orcid\":\"\",\"github\":\"https://github.com/ctbfl\",\"linkedin\":\"\",\"bilibili\":\"https://space.bilibili.com/398516133\",\"xiaohongshu\":\"\",\"twitter\":\"\",\"cv\":\"/JunshanHuang_CV.pdf\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"Embodied AI\",\"Kitchen Automation\",\"Dexterous Manipulation\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$L8\",\"about\",{\"content\":\"I am a PhD student at the School of Computer Science, Rutgers University, advised by [Prof. jingjin Yu](https://arc-l.github.io/).\\n\\nPrior to this, I obtained a BSc degree with Honor Degree (top 5%) in Artificial Intelligence from the University of Science and Technology of China (USTC).\\n\\nMy research focuses on developing more capable and reliable robot brains that allow robots to work efficiently and safely in household environments.\\n\\nThe kitchen, as a compact yet highly representative domestic space, serves as an ideal starting point for this mission.\\n\\nI categorize kitchen-related tasks into three major groups:\\n- Ingredient Preparation: Converting raw materials into ready-to-cook components.\\n- Food Production: Assisting in or autonomously performing cooking tasks.\\n- Post-Meal Tidying: Cleaning, dishwashing, and waste handling to maintain kitchen usability and hygiene.\\n\\nCurrently, I am focusing on solving post-meal tidying tasks.\",\"title\":\"About\"}],[\"$\",\"$L9\",\"featured_publications\",{\"publications\":[{\"id\":\"gao2025vlaos\",\"title\":\"VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models\",\"authors\":[{\"name\":\"Chongkai Gao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zixuan Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenghao Chi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junshan Huang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Fei\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiwen Hou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yuxuan Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yudi Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhirui Fang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lin Shao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Advances in Neural Information Processing Systems (NeurIPS)\",\"code\":\"https://github.com/HeegerGao/VLA-OS\",\"abstract\":\"$a\",\"description\":\"Unified benchmarking reveals visually grounded hierarchical planning excels in VLAs.\",\"selected\":true,\"preview\":\"VLA-OS.png\",\"bibtex\":\"$b\"},{\"id\":\"liang2023unifarn\",\"title\":\"Unifarn: Unified transformer for facial reaction generation\",\"authors\":[{\"name\":\"Cong Liang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiahe Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haofan Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bing Tang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junshan Huang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shangfei Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaoping Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags\",\"researchArea\":\"transformer-architectures\",\"journal\":\"\",\"conference\":\"Proceedings of the 31st ACM International Conference on Multimedia (ACM MM)\",\"pages\":\"9506--9510\",\"abstract\":\"We propose the Unified Transformer for Facial Reaction GeneratioN (UniFaRN) framework for facial reaction prediction in dyadic interactions. Given the video and audio of one side, the task is to generate facial reactions of the other side. The challenge of the task lies in the fusion of multi-modal inputs and balancing appropriateness and diversity. We adopt the Transformer architecture to tackle the challenge by leveraging its flexibility of handling multi-modal data and ability to control the generation process. By successfully capturing the correlations between multi-modal inputs and outputs with unified layers and balancing the performance with sampling methods, we have won first place in the REACT2023 challenge.\",\"description\":\"Unified transformer predicts expressive facial reactions from multimodal conversational input.\",\"selected\":true,\"preview\":\"unifarn.png\",\"bibtex\":\"$c\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":false}],[\"$\",\"$Ld\",\"news\",{\"items\":[{\"date\":\"2026-02\",\"content\":\"ðŸŽ‰ A paper was accepted by ICRA 2026!\"},{\"date\":\"2025-09\",\"content\":\"ðŸŽ‰ Our paper VLS-OS accepted by NeurIPS 2025!\"},{\"date\":\"2025-09\",\"content\":\"Starting my PhD at the Rutgers University~\"},{\"date\":\"2025-06\",\"content\":\"ðŸŽ“ Graduated from USTC with a honor degree! (Top 5%)\"},{\"date\":\"2025-06\",\"content\":\"Awarded the AI Talent Program Scholarship from USTC.\"},{\"date\":\"2023-10\",\"content\":\"Awarded the Shenzhen Stock Exchange Scholarship.\"},{\"date\":\"2023-10\",\"content\":\"We win the 1st place in the ACM REACT 2023 Multimodal Challenge!\"}],\"title\":\"News\"}]],false,false,false]}]]}]]}]}],[\"$\",\"$Le\",null,{\"children\":\"$Lf\"}],null,[\"$\",\"$L10\",null,{\"children\":[\"$L11\",\"$L12\",[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"AS1aASIa_ltT4anEliHID\",{\"children\":[[\"$\",\"$L15\",null,{\"children\":\"$L16\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$17\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"18:\"$Sreact.suspense\"\n19:I[4911,[],\"AsyncMetadata\"]\nf:[\"$\",\"$18\",null,{\"fallback\":null,\"children\":[\"$\",\"$L19\",null,{\"promise\":\"$@1a\"}]}]\n"])</script><script>self.__next_f.push([1,"12:null\n"])</script><script>self.__next_f.push([1,"16:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n11:null\n"])</script><script>self.__next_f.push([1,"1a:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Junshan Huang\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"PhD student at Rutgers University.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Junshan Huang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Junshan Huang,PhD,Research,Rutgers University\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Junshan Huang\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Junshan Huang\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Junshan Huang\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"PhD student at Rutgers University.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Junshan Huang's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Junshan Huang\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"PhD student at Rutgers University.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/fav.ico\"}]],\"error\":null,\"digest\":\"$undefined\"}\n14:{\"metadata\":\"$1a:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>