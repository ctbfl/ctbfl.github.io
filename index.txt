1:"$Sreact.fragment"
2:I[3719,["711","static/chunks/711-3ac098f4aa3de521.js","217","static/chunks/217-414f3bc2f28e17e5.js","862","static/chunks/862-15a393ad2462454b.js","177","static/chunks/app/layout-eb156f7f6ee3ed4b.js"],"ThemeProvider"]
3:I[768,["711","static/chunks/711-3ac098f4aa3de521.js","217","static/chunks/217-414f3bc2f28e17e5.js","862","static/chunks/862-15a393ad2462454b.js","177","static/chunks/app/layout-eb156f7f6ee3ed4b.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["711","static/chunks/711-3ac098f4aa3de521.js","217","static/chunks/217-414f3bc2f28e17e5.js","862","static/chunks/862-15a393ad2462454b.js","177","static/chunks/app/layout-eb156f7f6ee3ed4b.js"],"default"]
7:I[3583,["362","static/chunks/30a37ab2-4db9c7aaa3f9c255.js","204","static/chunks/94730671-6c060cb276896248.js","711","static/chunks/711-3ac098f4aa3de521.js","907","static/chunks/907-77394335ddd73061.js","217","static/chunks/217-414f3bc2f28e17e5.js","213","static/chunks/213-d1d263081572d555.js","997","static/chunks/997-38646f922bcecd56.js","748","static/chunks/748-7025ceaea1e8aacd.js","974","static/chunks/app/page-84185bc42e144a89.js"],"default"]
8:I[9507,["362","static/chunks/30a37ab2-4db9c7aaa3f9c255.js","204","static/chunks/94730671-6c060cb276896248.js","711","static/chunks/711-3ac098f4aa3de521.js","907","static/chunks/907-77394335ddd73061.js","217","static/chunks/217-414f3bc2f28e17e5.js","213","static/chunks/213-d1d263081572d555.js","997","static/chunks/997-38646f922bcecd56.js","748","static/chunks/748-7025ceaea1e8aacd.js","974","static/chunks/app/page-84185bc42e144a89.js"],"default"]
9:I[5218,["362","static/chunks/30a37ab2-4db9c7aaa3f9c255.js","204","static/chunks/94730671-6c060cb276896248.js","711","static/chunks/711-3ac098f4aa3de521.js","907","static/chunks/907-77394335ddd73061.js","217","static/chunks/217-414f3bc2f28e17e5.js","213","static/chunks/213-d1d263081572d555.js","997","static/chunks/997-38646f922bcecd56.js","748","static/chunks/748-7025ceaea1e8aacd.js","974","static/chunks/app/page-84185bc42e144a89.js"],"default"]
d:I[1990,["362","static/chunks/30a37ab2-4db9c7aaa3f9c255.js","204","static/chunks/94730671-6c060cb276896248.js","711","static/chunks/711-3ac098f4aa3de521.js","907","static/chunks/907-77394335ddd73061.js","217","static/chunks/217-414f3bc2f28e17e5.js","213","static/chunks/213-d1d263081572d555.js","997","static/chunks/997-38646f922bcecd56.js","748","static/chunks/748-7025ceaea1e8aacd.js","974","static/chunks/app/page-84185bc42e144a89.js"],"default"]
e:I[9665,[],"MetadataBoundary"]
10:I[9665,[],"OutletBoundary"]
13:I[4911,[],"AsyncMetadataOutlet"]
15:I[9665,[],"ViewportBoundary"]
17:I[6614,[],""]
:HL["/_next/static/css/85829fc21d34b09a.css","style"]
a:T5b6,Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.b:T7b6,@inproceedings{gao2025vlaos,
  title = {VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models},
  author = {Gao, Chongkai and Liu, Zixuan and Chi, Zhenghao and Huang, Junshan and Fei, Xin and Hou, Yiwen and Zhang, Yuxuan and Lin, Yudi and Fang, Zhirui and Shao, Lin},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2025},
  abstract = {Recent studies on Vision-Language-Action (VLA) models have shifted from the end-to-end action-generation paradigm toward a pipeline involving task planning followed by action generation, demonstrating improved performance on various complex, long-horizon manipulation tasks. However, existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved. To systematically investigate the impacts of different planning paradigms and representations isolating from network architectures and training data, in this paper, we introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands). Our results demonstrate that: 1) visually grounded planning representations are generally better than language planning representations; 2) the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds.},
  note = {San Diego, USA},
  project = {https://nus-lins-lab.github.io/vlaos/}
}c:T45b,@inproceedings{liang2023unifarn,
  title = {Unifarn: Unified transformer for facial reaction generation},
  author = {Liang, Cong and Wang, Jiahe and Zhang, Haofan and Tang, Bing and Huang, Junshan and Wang, Shangfei and Chen, Xiaoping},
  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia (ACM MM)},
  pages = {9506--9510},
  year = {2023},
  abstract = {We propose the Unified Transformer for Facial Reaction GeneratioN (UniFaRN) framework for facial reaction prediction in dyadic interactions. Given the video and audio of one side, the task is to generate facial reactions of the other side. The challenge of the task lies in the fusion of multi-modal inputs and balancing appropriateness and diversity. We adopt the Transformer architecture to tackle the challenge by leveraging its flexibility of handling multi-modal data and ability to control the generation process. By successfully capturing the correlations between multi-modal inputs and outputs with unified layers and balancing the performance with sampling methods, we have won first place in the REACT2023 challenge.}
}0:{"P":null,"b":"zs1EedYzA-mbxTUsxJE0T","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/85829fc21d34b09a.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"suppressHydrationWarning":true,"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Teaching","type":"page","target":"teaching","href":"/teaching"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"Blog","type":"page","target":"blog","href":"/blog"}],"siteTitle":"Junshan Huang","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"November 29, 2025"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Junshan Huang","title":"PhD Student","institution":"Rutgers University","avatar":"/hjs.jpg"},"social":{"email":"junshan.huang@rutgers.edu","location":"New Brunswick, NJ08901, USA","location_url":"https://maps.app.goo.gl/WQS1qSpAoB4bMgmKA","location_details":["CoRE Building, Busch Campus","Rutgers-New Brunswick, New Jersey, USA"],"google_scholar":"https://scholar.google.com/citations?user=bPL1D18AAAAJ&hl=en","orcid":"","github":"https://github.com/ctbfl","linkedin":"","bilibili":"https://space.bilibili.com/398516133","xiaohongshu":"","twitter":"","cv":"/junshanhuang_CV.pdf"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["Embodied AI","Kitchen Automation","Dexterous Manipulation"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"I am a PhD student at the School of Computer Science, Rutgers University, advised by [Prof. jingjin Yu](https://arc-l.github.io/).\r\n\r\nPrior to this, I obtained a BSc degree with Honor Degree (top 5%) in Artificial Intelligence from the University of Science and Technology of China (USTC).\r\n\r\nMy research focuses on developing more capable and reliable robot brains that allow robots to work efficiently and safely in household environments.\r\n\r\nThe kitchen, as a compact yet highly representative domestic space, serves as an ideal starting point for this mission.\r\n\r\nI categorize kitchen-related tasks into three major groups:\r\n- Ingredient Preparation: Converting raw materials into ready-to-cook components.\r\n- Food Production: Assisting in or autonomously performing cooking tasks.\r\n- Post-Meal Tidying: Cleaning, dishwashing, and waste handling to maintain kitchen usability and hygiene.\r\n\r\nCurrently, I am focusing on solving post-meal tidying tasks.","title":"About"}],["$","$L9","featured_publications",{"publications":[{"id":"gao2025vlaos","title":"VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models","authors":[{"name":"Chongkai Gao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zixuan Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenghao Chi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junshan Huang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Xin Fei","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yiwen Hou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yuxuan Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yudi Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhirui Fang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lin Shao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"Advances in Neural Information Processing Systems (NeurIPS)","code":"https://github.com/HeegerGao/VLA-OS","abstract":"$a","description":"Unified benchmarking reveals visually grounded hierarchical planning excels in VLAs.","selected":true,"preview":"VLA-OS.png","bibtex":"$b"},{"id":"liang2023unifarn","title":"Unifarn: Unified transformer for facial reaction generation","authors":[{"name":"Cong Liang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiahe Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Haofan Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bing Tang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Junshan Huang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Shangfei Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaoping Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"transformer-architectures","journal":"","conference":"Proceedings of the 31st ACM International Conference on Multimedia (ACM MM)","pages":"9506--9510","abstract":"We propose the Unified Transformer for Facial Reaction GeneratioN (UniFaRN) framework for facial reaction prediction in dyadic interactions. Given the video and audio of one side, the task is to generate facial reactions of the other side. The challenge of the task lies in the fusion of multi-modal inputs and balancing appropriateness and diversity. We adopt the Transformer architecture to tackle the challenge by leveraging its flexibility of handling multi-modal data and ability to control the generation process. By successfully capturing the correlations between multi-modal inputs and outputs with unified layers and balancing the performance with sampling methods, we have won first place in the REACT2023 challenge.","description":"Unified transformer predicts expressive facial reactions from multimodal conversational input.","selected":true,"preview":"unifarn.png","bibtex":"$c"}],"title":"Selected Publications","enableOnePageMode":false}],["$","$Ld","news",{"items":[{"date":"2025-09","content":"ðŸŽ‰ Our paper VLS-OS accepted by NeurIPS 2025!"},{"date":"2025-09","content":"Starting my PhD at the Rutgers University~"},{"date":"2025-06","content":"ðŸŽ“ Graduated from USTC with a honor degree! (Top 5%)"},{"date":"2025-06","content":"Awarded the AI Talent Program Scholarship from USTC."},{"date":"2023-10","content":"Awarded the Shenzhen Stock Exchange Scholarship."},{"date":"2023-10","content":"We win the 1st place in the ACM REACT 2023 Multimodal Challenge!"}],"title":"News"}]],false,false,false]}]]}]]}]}],["$","$Le",null,{"children":"$Lf"}],null,["$","$L10",null,{"children":["$L11","$L12",["$","$L13",null,{"promise":"$@14"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","8EVlOSVzFd9OBGMKW2Swx",{"children":[["$","$L15",null,{"children":"$L16"}],null]}],null]}],false]],"m":"$undefined","G":["$17","$undefined"],"s":false,"S":true}
18:"$Sreact.suspense"
19:I[4911,[],"AsyncMetadata"]
f:["$","$18",null,{"fallback":null,"children":["$","$L19",null,{"promise":"$@1a"}]}]
12:null
16:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
11:null
1a:{"metadata":[["$","title","0",{"children":"Junshan Huang"}],["$","meta","1",{"name":"description","content":"PhD student at Rutgers University."}],["$","meta","2",{"name":"author","content":"Junshan Huang"}],["$","meta","3",{"name":"keywords","content":"Junshan Huang,PhD,Research,Rutgers University"}],["$","meta","4",{"name":"creator","content":"Junshan Huang"}],["$","meta","5",{"name":"publisher","content":"Junshan Huang"}],["$","meta","6",{"property":"og:title","content":"Junshan Huang"}],["$","meta","7",{"property":"og:description","content":"PhD student at Rutgers University."}],["$","meta","8",{"property":"og:site_name","content":"Junshan Huang's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Junshan Huang"}],["$","meta","13",{"name":"twitter:description","content":"PhD student at Rutgers University."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
14:{"metadata":"$1a:metadata","error":null,"digest":"$undefined"}
